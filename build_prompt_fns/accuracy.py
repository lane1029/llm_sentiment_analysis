# prompt template
def build_prompt(question, answer):
    prompt = f"""
        --- Role:
        You are a sentiment analyzer tasked with evaluating the interviewee’s sentiment about the perceived *accuracy* of PROJECT A and only PROJECT A. You are analyzing excerpts from an interview in question and answer pairs.

        You will assess sentiment toward the following construct:
        a) Perceived accuracy of PROJECT A
            - Definition: Accuracy refers to the extent to which the interviewee believes that PROJECT A produces correct, reliable, precise, and error-free results. This includes whether they feel confident in the outputs, decisions, or information generated by PROJECT A.
            - Options:
                • *Positive*: The interviewee clearly expresses that PROJECT A is accurate, reliable, or produces correct results.
                • *Negative*: The interviewee clearly expresses that PROJECT A is inaccurate, unreliable, or prone to errors.
                • *Neutral*: The interviewee expresses a mixed, mild, or indifferent view about its accuracy.
                • *Not_mentioned*: The answer does not clearly reference accuracy, PROJECT A, or is ambiguous.

        --- question:
        ```
        {question}
        ```

        --- answer:
        ```
        {answer}
        ```

        Please respond in JSON format with these fields:
            - accuracy_sentiment
            - accuracy_sentiment_confidence_score
            - accuracy_sentiment_evidence

        Use the following as options:
        --- Options:
            accuracy_sentiment: [positive, negative, neutral, not_mentioned]

        For the evidence, provide a concise explanation of your assessment, including direct quotes where appropriate.

        Examples:
        Question: "Do you trust the results that PROJECT A gives you?"
        Answer: "Yes, it’s very precise and I haven’t seen it make mistakes."
        Output:
        {{
        "accuracy_sentiment": "positive",
        "accuracy_sentiment_confidence_score": 0.95,
        "accuracy_sentiment_evidence": "The interviewee says 'it’s very precise' and 'haven’t seen it make mistakes,' indicating confidence in accuracy."
        }}

        Question: "How accurate do you think PROJECT A is?"
        Answer: "It often gives wrong results, and I don’t really trust it."
        Output:
        {{
        "accuracy_sentiment": "negative",
        "accuracy_sentiment_confidence_score": 0.95,
        "accuracy_sentiment_evidence": "The interviewee states 'gives wrong results' and 'don’t really trust it,' indicating negative perception of accuracy."
        }}

        Question: "Do you believe PROJECT A provides reliable results?"
        Answer: "Sometimes it seems right, other times I’m not sure."
        Output:
        {{
        "accuracy_sentiment": "neutral",
        "accuracy_sentiment_confidence_score": 0.9,
        "accuracy_sentiment_evidence": "The interviewee notes both confidence and doubt, suggesting a mixed view on accuracy."
        }}

        Question: "How do you feel about machine learning?"
        Answer: "It’s fine, I am concerned about the accuracy and reliability of machine learning products, but I am willing to give them a try."
        Output:
        {{
        "accuracy_sentiment": "not_mentioned",
        "accuracy_sentiment_confidence_score": 0.0,
        "accuracy_sentiment_evidence": "The interviewee does not clearly reference PROJECT A."
        }}
    """

    return prompt
